<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>7-27日报 python爬虫实战练习 | 王岚婷的博客</title><meta name="description" content="7-27日报 python爬虫实战练习"><meta name="keywords" content="python"><meta name="author" content="王岚婷"><meta name="copyright" content="王岚婷"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2019/07/27/7-27日报 python爬虫实战练习/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="7-27日报 python爬虫实战练习"><meta name="twitter:description" content="7-27日报 python爬虫实战练习"><meta name="twitter:image" content="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b10000_10000&amp;sec=1563544003&amp;di=1a47abf2b579d8d072a1392c80d244f9&amp;src=http://img3.duitang.com/uploads/item/201412/06/20141206031503_5rVyE.jpeg"><meta property="og:type" content="article"><meta property="og:title" content="7-27日报 python爬虫实战练习"><meta property="og:url" content="http://yoursite.com/2019/07/27/7-27日报 python爬虫实战练习/"><meta property="og:site_name" content="王岚婷的博客"><meta property="og:description" content="7-27日报 python爬虫实战练习"><meta property="og:image" content="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b10000_10000&amp;sec=1563544003&amp;di=1a47abf2b579d8d072a1392c80d244f9&amp;src=http://img3.duitang.com/uploads/item/201412/06/20141206031503_5rVyE.jpeg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="7-28日报 python进程" href="http://yoursite.com/2019/07/28/7-28日报 进程/"><link rel="next" title="python爬虫入门" href="http://yoursite.com/2019/07/26/python爬虫入门/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://jerryc.me/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#7-27周报-python爬虫"><span class="toc-number">1.</span> <span class="toc-text">7-27周报   python爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么需要selenium"><span class="toc-number">1.1.</span> <span class="toc-text">为什么需要selenium</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#安装环境"><span class="toc-number">1.2.</span> <span class="toc-text">安装环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取一个小说网站"><span class="toc-number">1.3.</span> <span class="toc-text">爬取一个小说网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实战项目"><span class="toc-number">1.4.</span> <span class="toc-text">实战项目</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/post.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">王岚婷的博客</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b10000_10000&amp;sec=1563544003&amp;di=1a47abf2b579d8d072a1392c80d244f9&amp;src=http://img3.duitang.com/uploads/item/201412/06/20141206031503_5rVyE.jpeg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description">计算机小白</div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">7-27日报 python爬虫实战练习</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-07-27<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-09-10</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python/">python</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="7-27周报-python爬虫"><a href="#7-27周报-python爬虫" class="headerlink" title="7-27周报   python爬虫"></a>7-27周报   python爬虫</h1><h2 id="为什么需要selenium"><a href="#为什么需要selenium" class="headerlink" title="为什么需要selenium"></a>为什么需要selenium</h2><p>许多网页是通过JS动态加载的，selenium是自动化测试工具，支持多种浏览器，主要用来解决爬虫渲染的问题</p>
<h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install selenium</span><br></pre></td></tr></table></figure>

<p>以及去官网下载chrome driver用来打开浏览器<br>emmmm….教程可以看<a href="https://www.cnblogs.com/zhaof/p/6953241.html" target="_blank" rel="noopener">这里</a></p>
<h2 id="爬取一个小说网站"><a href="#爬取一个小说网站" class="headerlink" title="爬取一个小说网站"></a>爬取一个小说网站</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import requests,sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class dowmloador:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.completeurl=&apos;http://www.biqukan.com/&apos;</span><br><span class="line">        self.tmpurl=&quot;https://www.biqukan.com/1_1094/&quot;</span><br><span class="line">        self.nums=0</span><br><span class="line">        self.urls=[]</span><br><span class="line">        self.name=[]</span><br><span class="line">    def downloadUrl(self):</span><br><span class="line">        html=requests.get(url=self.tmpurl)</span><br><span class="line">        html.encoding=&quot;gb2312&quot;</span><br><span class="line">        bs=BeautifulSoup(html.text,&apos;html.parser&apos;)</span><br><span class="line">        content=bs.find_all(&quot;div&quot;,class_=&quot;listmain&quot;)</span><br><span class="line">        newbs=BeautifulSoup(str(content[0]),&apos;html.parser&apos;)</span><br><span class="line">        allUrl=newbs.find_all(&quot;a&quot;)</span><br><span class="line">        self.nums=len(allUrl[15:])</span><br><span class="line">        for tmp in allUrl[15:]:</span><br><span class="line">            self.urls.append(self.completeurl+str(tmp[&apos;href&apos;]))</span><br><span class="line">            self.name.append(tmp.string)</span><br><span class="line">    def getContent(self,index):</span><br><span class="line">        html=requests.get(self.urls[index])</span><br><span class="line">        html.encoding=&quot;gb2312&quot;</span><br><span class="line">        bs=BeautifulSoup(html.text)</span><br><span class="line">        content=bs.find_all(&quot;div&quot;,class_=&quot;showtxt&quot;)</span><br><span class="line">        return content[0].text.replace(&apos;&lt;/br&gt;&apos;,&apos;\n&apos;)</span><br><span class="line">    def writer(self,name,path,text):</span><br><span class="line">        write__flag=True</span><br><span class="line">        f=open(path,mode=&apos;a&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line">        f.write(name)</span><br><span class="line">        f.writelines(text)</span><br><span class="line">        f.write(&apos;\n\n&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dl=dowmloador()</span><br><span class="line">dl.downloadUrl()</span><br><span class="line"></span><br><span class="line">print(&quot;开始下载。。。&quot;)</span><br><span class="line"></span><br><span class="line">for i in range(dl.nums):</span><br><span class="line">    dl.writer(dl.name[i],r&quot;D:\下载\a.txt&quot;,dl.getContent(i))</span><br><span class="line">    sys.stdout.write(&quot;  已下载:%.3f%%&quot; %  float(i/dl.nums) + &apos;\r&apos;)</span><br><span class="line">    sys.stdout.flush()</span><br><span class="line">print(&apos;下载完成&apos;)</span><br></pre></td></tr></table></figure>

<h2 id="实战项目"><a href="#实战项目" class="headerlink" title="实战项目"></a>实战项目</h2><p>利用正则表达式和requests库来抓取猫眼电影top100数据并保存，写入时将ensure_ascii=False，然后将open中的encoding设置为utf-8能够输出正确的中文字符</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line">import json</span><br><span class="line">from multiprocessing import Pool </span><br><span class="line">import requests</span><br><span class="line">import re</span><br><span class="line">from requests.exceptions import RequestException</span><br><span class="line"></span><br><span class="line">def get_page(url):</span><br><span class="line">    try:</span><br><span class="line">        response=requests.get(url)</span><br><span class="line">        if(response.status_code==200):</span><br><span class="line">            return response.text</span><br><span class="line">        else :</span><br><span class="line">            return None</span><br><span class="line">    except RequestException:</span><br><span class="line">        return None</span><br><span class="line">def analysis(html):</span><br><span class="line">    pattern=re.compile(&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d)&lt;/i&gt;.*?title=&quot;(.*?)&quot;.*?&lt;img.*?alt=.*?src=&quot;(.*?)&quot;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p&gt;&apos;+</span><br><span class="line">        &apos;.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos;,re.S)</span><br><span class="line">    </span><br><span class="line">    content=re.findall(pattern,html)</span><br><span class="line">    print(content)</span><br><span class="line">    for temp in content:</span><br><span class="line">        yield &#123;</span><br><span class="line">            &quot;index&quot;:temp[0],</span><br><span class="line">            &quot;name&quot;:temp[1],</span><br><span class="line">            &quot;img&quot;:temp[2],</span><br><span class="line">            &quot;actors&quot;:temp[3].strip()[3:],</span><br><span class="line">            &quot;time&quot;:temp[4],</span><br><span class="line">            &quot;score&quot;:temp[5]+temp[6]</span><br><span class="line">        &#125;</span><br><span class="line">   </span><br><span class="line">def write_to_file(content):</span><br><span class="line">    with open(&quot;result1.txt&quot;,&quot;a&quot;,encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        f.write(json.dumps(content,ensure_ascii=False)+&apos;\n&apos;)</span><br><span class="line">        f.close()</span><br><span class="line">def main(offest):</span><br><span class="line">    url=&quot;https://maoyan.com/board/4&quot;</span><br><span class="line">    html=get_page(url+&quot;?offset=&quot;+str(offest))</span><br><span class="line">    for items in analysis(html):</span><br><span class="line">            write_to_file(items)</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    for temp in range(10):</span><br><span class="line">        html=get_page(url+&quot;?offset=&quot;+str(temp*10))</span><br><span class="line">        for items in analysis(html):</span><br><span class="line">            write_to_file(items)</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">    #利用线程池抓取数据，提升速度</span><br><span class="line">    pool=Pool()</span><br><span class="line">    pool.map(main,[i*10 for i in range(10)])</span><br></pre></td></tr></table></figure>

</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">王岚婷</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/07/27/7-27日报 python爬虫实战练习/">http://yoursite.com/2019/07/27/7-27日报 python爬虫实战练习/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://condyboy.github.io">null</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python    </a></div><div class="post_share"><div class="social-share" data-image="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b10000_10000&amp;sec=1563544003&amp;di=1a47abf2b579d8d072a1392c80d244f9&amp;src=http://img3.duitang.com/uploads/item/201412/06/20141206031503_5rVyE.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" data-src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" data-src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/07/28/7-28日报 进程/"><img class="prev_cover lozad" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1563553510209&amp;di=e8001f55819fa5b11ec4e20501b6f99a&amp;imgtype=0&amp;src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>7-28日报 python进程</span></div></a></div><div class="next-post pull-right"><a href="/2019/07/26/python爬虫入门/"><img class="next_cover lozad" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1563553510209&amp;di=e8001f55819fa5b11ec4e20501b6f99a&amp;imgtype=0&amp;src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>python爬虫入门</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/07/21/7-21日报 第一个python爬虫/" title="7-21日报 第一个python爬虫"><img class="relatedPosts_cover lozad" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563553510209&di=e8001f55819fa5b11ec4e20501b6f99a&imgtype=0&src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg"><div class="relatedPosts_title">7-21日报 第一个python爬虫</div></a></div><div class="relatedPosts_item"><a href="/2019/07/24/7-24日报 自定义排序以及lambda和filter/" title="7-24日报 自定义排序以及lambda和filter"><img class="relatedPosts_cover lozad" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563553510209&di=e8001f55819fa5b11ec4e20501b6f99a&imgtype=0&src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg"><div class="relatedPosts_title">7-24日报 自定义排序以及lambda和filter</div></a></div><div class="relatedPosts_item"><a href="/2019/07/26/7-26日报 装饰器/" title="7-26日报 装饰器"><img class="relatedPosts_cover lozad" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563553510209&di=e8001f55819fa5b11ec4e20501b6f99a&imgtype=0&src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg"><div class="relatedPosts_title">7-26日报 装饰器</div></a></div><div class="relatedPosts_item"><a href="/2019/07/25/7-25日报 可变参数和关键字参数/" title="7-25日报 可变参数和关键字参数"><img class="relatedPosts_cover lozad" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563553510209&di=e8001f55819fa5b11ec4e20501b6f99a&imgtype=0&src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg"><div class="relatedPosts_title">7-25日报 可变参数和关键字参数</div></a></div><div class="relatedPosts_item"><a href="/2019/07/30/7-30日报 内置函数datetime/" title="7-30日报 内置函数datetime"><img class="relatedPosts_cover lozad" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563553510209&di=e8001f55819fa5b11ec4e20501b6f99a&imgtype=0&src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg"><div class="relatedPosts_title">7-30日报 内置函数datetime</div></a></div><div class="relatedPosts_item"><a href="/2019/07/26/python爬虫入门/" title="python爬虫入门"><img class="relatedPosts_cover lozad" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563553510209&di=e8001f55819fa5b11ec4e20501b6f99a&imgtype=0&src=http%3A%2F%2Fpic44.nipic.com%2F20140729%2F19317756_161533101860_2.jpg"><div class="relatedPosts_title">python爬虫入门</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By 王岚婷</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">簡</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>